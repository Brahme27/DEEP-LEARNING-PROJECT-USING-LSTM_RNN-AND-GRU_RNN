{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\zaynb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Data Collection\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "import pandas as pd  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "data=gutenberg.raw('shakespeare-hamlet.txt')\n",
    "# save to a file \n",
    "with open('hamlet.txt','w') as file:\n",
    "    file.write(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#load the dataset\n",
    "with open('hamlet.txt','r') as file:\n",
    "    text=file.read().lower()\n",
    "\n",
    "#Tokenize the Text- Creating indexes for words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4818"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create input sequences \n",
    "input_sequences=[]\n",
    "for line in text.split('\\n'):\n",
    "    token_list=tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence=token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sentence it starts creating bigram --- ngram \n",
    "#input_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequence ensure to have same length of vectors to all the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence=max([len(x) for x in input_sequences])\n",
    "max_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences=np.array(pad_sequences(input_sequences,maxlen=max_sequence,padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    1,  687],\n",
       "       [   0,    0,    0, ...,    1,  687,    4],\n",
       "       [   0,    0,    0, ...,  687,    4,   45],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4,   45, 1047],\n",
       "       [   0,    0,    0, ...,   45, 1047,    4],\n",
       "       [   0,    0,    0, ..., 1047,    4,  193]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Predictors and labels \n",
    "import tensorflow as tf \n",
    "X,y=input_sequences[:,:-1],input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0,    1],\n",
       "       [   0,    0,    0, ...,    0,    1,  687],\n",
       "       [   0,    0,    0, ...,    1,  687,    4],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  687,    4,   45],\n",
       "       [   0,    0,    0, ...,    4,   45, 1047],\n",
       "       [   0,    0,    0, ...,   45, 1047,    4]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 687,    4,   45, ..., 1047,    4,  193])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25732, 13) (25732, 4818)\n"
     ]
    }
   ],
   "source": [
    "y=tf.keras.utils.to_categorical(y,num_classes=total_words)\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4818"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20585, 13)\n",
      "y_train shape: (20585, 4818)\n",
      "X_test shape: (5147, 13)\n",
      "y_test shape: (5147, 4818)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN TEST SPLIT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define early stopping \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5,restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  \\nLayer\\tParameters\\tReturn Type\\tPurpose\\nEmbedding\\ttotal_words, 100, input_length\\t(batch_size, input_length, 100)\\tConverts indices to dense word vectors\\nLSTM (1st)\\t100, return_sequences=True\\t(batch_size, input_length, 100)\\tCaptures dependencies between words\\nDropout\\t0.2\\tSame as input\\tPrevents overfitting\\nLSTM (2nd)\\t50\\t(batch_size, 50)\\tSummarizes the sequence in one vector\\nDense\\ttotal_words, activation='softmax'\\t(batch_size, total_words)\\tPredicts next word as probabilities\\n\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''  \n",
    "Layer\tParameters\tReturn Type\tPurpose\n",
    "Embedding\ttotal_words, 100, input_length\t(batch_size, input_length, 100)\tConverts indices to dense word vectors\n",
    "LSTM (1st)\t100, return_sequences=True\t(batch_size, input_length, 100)\tCaptures dependencies between words\n",
    "Dropout\t0.2\tSame as input\tPrevents overfitting\n",
    "LSTM (2nd)\t50\t(batch_size, 50)\tSummarizes the sequence in one vector\n",
    "Dense\ttotal_words, activation='softmax'\t(batch_size, total_words)\tPredicts next word as probabilities\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4818"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 13, 100)           481800    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 13, 100)           80400     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 13, 100)           0         \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4818)              486618    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1129218 (4.31 MB)\n",
      "Trainable params: 1129218 (4.31 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
    "\n",
    "\n",
    "## Define the model \n",
    "model=Sequential()\n",
    "model.add(Embedding(total_words,100,input_length=max_sequence-1))\n",
    "model.add(LSTM(100,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words,activation='softmax'))\n",
    "\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my system is not a very powerful one,thats why im going to run for only 5 epochs \n",
    "# for better accuracy you can run more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4817"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "644/644 [==============================] - 38s 49ms/step - loss: 6.9059 - accuracy: 0.0319 - val_loss: 6.7330 - val_accuracy: 0.0346\n",
      "Epoch 2/5\n",
      "644/644 [==============================] - 29s 46ms/step - loss: 6.4732 - accuracy: 0.0377 - val_loss: 6.8213 - val_accuracy: 0.0404\n",
      "Epoch 3/5\n",
      "644/644 [==============================] - 30s 47ms/step - loss: 6.3307 - accuracy: 0.0449 - val_loss: 6.8383 - val_accuracy: 0.0484\n",
      "Epoch 4/5\n",
      "644/644 [==============================] - 30s 47ms/step - loss: 6.1734 - accuracy: 0.0516 - val_loss: 6.9087 - val_accuracy: 0.0519\n",
      "Epoch 5/5\n",
      "644/644 [==============================] - 30s 47ms/step - loss: 6.0354 - accuracy: 0.0575 - val_loss: 6.9369 - val_accuracy: 0.0583\n"
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "history=model.fit(X_train,y_train,epochs=5,validation_data=(X_test,y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to predict the next word\n",
    "def predict_next_word(model,tokenizer,text,max_sequence_len):\n",
    "    token_list=tokenizer.texts_to_sequences([text])[0]\n",
    "    if len(token_list)>=max_sequence_len:\n",
    "        token_list=token_list[-(max_sequence_len-1):]\n",
    "    token_list=pad_sequences([token_list],maxlen=max_sequence_len-1,padding='pre',)\n",
    "    predicted=model.predict(token_list,verbose=0)\n",
    "    predicted_word_index=np.argmax(predicted,axis=1)\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index==predicted_word_index:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:the\n",
      "Next word: king\n"
     ]
    }
   ],
   "source": [
    "input_text='the'\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1 \n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
    "print(f\"Next word: {next_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\END TO END DEEP LEARNING PROJECT USING LSTM-GRU RNN\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('next_word_LSTM.h5')\n",
    "import pickle \n",
    "with open('tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly you can also use GRU RNN for that you just import GRU from tensorflow\n",
    "# and replace LSTM from GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
